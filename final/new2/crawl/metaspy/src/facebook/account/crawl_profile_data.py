import os
import json
import glob
import sys
from time import sleep
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException
from rich import print as rprint
from typing import Dict, List, Any
import pickle
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
import re

rprint("[bold cyan]B·∫Øt ƒë·∫ßu qu√° tr√¨nh kh·ªüi t·∫°o...[/bold cyan]")

current_dir = os.path.dirname(os.path.abspath(__file__))
src_dir = os.path.dirname(os.path.dirname(os.path.dirname(current_dir)))
project_root = os.path.dirname(src_dir)
if src_dir not in sys.path:
    sys.path.append(src_dir)

try:
    from ..facebook_base import BaseFacebookScraper
except (ImportError, ValueError):
    from metaspy.src.facebook.facebook_base import BaseFacebookScraper

class CustomBaseFacebookScraper(BaseFacebookScraper):
    def __init__(self, user_id: str, base_url: str):
        chrome_options = Options()
        chrome_options.add_argument("--headless")
        chrome_options.add_argument('--disable-gpu')
        chrome_options.add_argument('--no-sandbox')
        chrome_options.add_argument('--disable-dev-shm-usage')
        chrome_options.add_argument('--window-size=1920,1080')
        chrome_options.add_argument('--start-maximized')
        
        self._driver = webdriver.Chrome(options=chrome_options)
        super().__init__(user_id, base_url)

    def _find_cookie_file(self) -> str:
        possible_paths = [
            os.path.join(project_root, "cookies.json"),
            os.path.join(src_dir, "cookies.json"),
            os.path.join(current_dir, "cookies.json"),
            os.path.join(os.path.dirname(current_dir), "cookies.json"),
        ]
        
        for path in possible_paths:
            rprint(f"- {path}")
            if os.path.exists(path):
                rprint(f"[green]T√¨m th·∫•y file cookie t·∫°i: {path}[/green]")
                return path
        
        raise FileNotFoundError(f"Kh√¥ng t√¨m th·∫•y file cookie ·ªü c√°c v·ªã tr√≠:\n" + "\n".join(f"- {p}" for p in possible_paths))

    def _load_cookies(self) -> None:
        try:
            self._driver.get("https://www.facebook.com")
            sleep(2)
            
            self._driver.delete_all_cookies()
            cookie_path = self._find_cookie_file()
            
            with open(cookie_path, "rb") as file:
                cookies = pickle.load(file)
                for cookie in cookies:
                    try:
                        self._driver.add_cookie(cookie)
                    except Exception as e:
                        rprint(f"[red]L·ªói khi th√™m cookie: {e}[/red]")
            
            rprint("[green]ƒê√£ load cookies th√†nh c√¥ng[/green]")
            self._driver.refresh()
            sleep(2)

        except Exception as e:
            rprint(f"[red]L·ªói khi load cookies: {e}[/red]")
            raise

    def _load_cookies_and_refresh_driver(self) -> None:
        self._load_cookies()

class FacebookProfileCrawler(CustomBaseFacebookScraper):
    def __init__(self, user_id: str):
        self.user_id = user_id
        
        if user_id.isdigit():
            self.profile_url = f"https://www.facebook.com/profile.php?id={user_id}"
            base_url = self.profile_url
        else:
            self.profile_url = f"https://www.facebook.com/{user_id}"
            base_url = self.profile_url
            
        super().__init__(user_id, base_url)
        self.wait = WebDriverWait(self._driver, 10)

    def get_profile_picture(self):
        rprint("[cyan]‚Üí ƒêang l·∫•y ·∫£nh ƒë·∫°i di·ªán...[/cyan]")
        try:
            avatar_element = self.wait.until(
                EC.presence_of_element_located((By.CSS_SELECTOR, "image[preserveAspectRatio='xMidYMid slice']"))
            )
            url = avatar_element.get_attribute("xlink:href")
            rprint(f"[green]  ‚úì ƒê√£ l·∫•y ƒë∆∞·ª£c URL ·∫£nh ƒë·∫°i di·ªán[/green]")
            return url
        except:
            rprint("[yellow]  ! Kh√¥ng l·∫•y ƒë∆∞·ª£c link ·∫£nh ƒë·∫°i di·ªán[/yellow]")
            return None

    def get_cover_photo(self):
        rprint("[cyan]‚Üí ƒêang l·∫•y ·∫£nh b√¨a...[/cyan]")
        try:
            cover_element = self.wait.until(
                EC.presence_of_element_located((By.CSS_SELECTOR, "img[data-imgperflogname='profileCoverPhoto']"))
            )
            url = cover_element.get_attribute("src")
            rprint(f"[green]  ‚úì ƒê√£ l·∫•y ƒë∆∞·ª£c URL ·∫£nh b√¨a[/green]")
            return url
        except:
            rprint("[yellow]  ! Kh√¥ng l·∫•y ƒë∆∞·ª£c link ·∫£nh b√¨a[/yellow]")
            return None

    def get_all_profile_text(self):
        try:
            all_text_elements = self._driver.find_elements(
                By.XPATH,
                "//span[contains(@class, 'x193iq5w') or contains(@class, 'xt0psk2') or contains(@class, 'x1heor9g')]"
            )
            return [element.text.strip() for element in all_text_elements if element.text.strip()]
        except Exception as e:
            rprint(f"[yellow]  ! L·ªói khi l·∫•y text t·ª´ trang profile: {str(e)}[/yellow]")
            return []

    def get_followers_count(self):
        rprint("[cyan]‚Üí ƒêang l·∫•y s·ªë ng∆∞·ªùi theo d√µi...[/cyan]")
        try:
            all_texts = self.get_all_profile_text()
            
            for text in all_texts:
                if any(c.isdigit() for c in text) and ("ng∆∞·ªùi theo d√µi" in text.lower() or "followers" in text.lower()):
                    if "‚Ä¢" in text:
                        text = text.split("‚Ä¢")[0].strip()
                    
                    rprint(f"[green]  ‚úì S·ªë ng∆∞·ªùi theo d√µi: {text}[/green]")
                    return text.strip()
                        
            rprint("[yellow]  ! Kh√¥ng t√¨m th·∫•y s·ªë ng∆∞·ªùi theo d√µi[/yellow]")
            return "0"
        except Exception as e:
            rprint(f"[yellow]  ! L·ªói khi l·∫•y s·ªë ng∆∞·ªùi theo d√µi: {str(e)}[/yellow]")
            return "0"

    def get_friends_count(self):
        rprint("[cyan]‚Üí ƒêang l·∫•y s·ªë l∆∞·ª£ng b·∫°n b√®...[/cyan]")
        try:
            all_texts = self.get_all_profile_text()
            
            for text in all_texts:
                if ("b·∫°n b√®" in text.lower() or "friends" in text.lower() or "ng∆∞·ªùi b·∫°n" in text.lower()) and any(c.isdigit() for c in text):
                    rprint(f"[green]  ‚úì S·ªë b·∫°n b√®: {text}[/green]")
                    return text.strip()
                        
            rprint("[yellow]  ! Kh√¥ng t√¨m th·∫•y s·ªë b·∫°n b√®[/yellow]")
            return "0"
        except Exception as e:
            rprint(f"[yellow]  ! L·ªói khi l·∫•y s·ªë b·∫°n b√®: {str(e)}[/yellow]")
            return "0"
    def get_name(self):
        rprint("[cyan]‚Üí ƒêang l·∫•y t√™n Facebook...[/cyan]")
        try:
            name_element = self.wait.until(
                EC.presence_of_element_located((By.CSS_SELECTOR, "h1.x1heor9g"))
            )
            name = name_element.text.strip()
            if name == "":
                raise Exception("Name is null")
            rprint(f"[green]  ‚úì ƒê√£ l·∫•y ƒë∆∞·ª£c t√™n Facebook: {name}[/green]")
            return name
        except:
            sleep(5)
            name_element = self._driver.find_elements(
                By.CSS_SELECTOR,
                "h1.x1heor9g, span.x193iq5w.xeuugli.x13faqbe.x1vvkbs.x1xmvt09.x1lliihq.x1s928wv.xhkezso.x1gmr53x.x1cpjm7i.x1fgarty.x1943h6x.x4zkp8e.x676frb.x1nxh6w3.x1sibtaa.x1s688f.xzsf02u"
            )
            if len(name_element) >= 3:
                #rprint(f"{name_element[0]}")
                name = name_element[2].text.strip()
                rprint(f"[green]  ‚úì ƒê√£ l·∫•y ƒë∆∞·ª£c t√™n Facebook: {name}[/green]")
                return name
        rprint("[yellow]  ! Kh√¥ng l·∫•y ƒë∆∞·ª£c t√™n Facebook[/yellow]")
        return None
    def get_bio(self):
        rprint("[cyan]‚Üí ƒêang l·∫•y ti·ªÉu s·ª≠...[/cyan]")
        try:
            bio_selectors = [
                "//div[contains(@class, 'x1heor9g')]//div[contains(text(), 'Gi·ªõi thi·ªáu')]/..",
                "//div[contains(@class, 'x1heor9g')]//div[contains(text(), 'Bio')]/..",
                "//div[contains(@class, 'xyamay9')]//div[contains(text(), 'Gi·ªõi thi·ªáu')]/..",
                "//div[contains(@class, 'x1cy8zhl')]//div[contains(text(), 'Gi·ªõi thi·ªáu')]/.."
            ]
            
            for selector in bio_selectors:
                try:
                    bio_element = self._driver.find_element(By.XPATH, selector)
                    bio_text = bio_element.text.strip()
                    if bio_text and "Gi·ªõi thi·ªáu" in bio_text:
                        rprint(f"[green]  ‚úì ƒê√£ l·∫•y ƒë∆∞·ª£c ti·ªÉu s·ª≠ t·ª´ trang profile: {bio_text}[/green]")
                        return bio_text
                except:
                    continue

            rprint("[cyan]  ‚Üí Th·ª≠ t√¨m trong trang about...[/cyan]")
            about_url = f"{self.profile_url}/about" if not self.user_id.isdigit() else f"{self.profile_url}&sk=about"
            rprint(f"[blue]  - Truy c·∫≠p: {about_url}[/blue]")
            self._driver.get(about_url)
            sleep(1)
            
            about_selectors = [
                "//div[contains(@class, 'x1heor9g')]//div[contains(text(), 'Ti·ªÉu s·ª≠')]/following-sibling::div",
                "//div[contains(@class, 'xyamay9')]//div[contains(text(), 'Ti·ªÉu s·ª≠')]/following-sibling::div",
                "//div[contains(@class, 'x1cy8zhl')]//div[contains(text(), 'Ti·ªÉu s·ª≠')]/following-sibling::div",
                "//div[contains(text(), 'Bio')]/following-sibling::div"
            ]
            
            for selector in about_selectors:
                try:
                    bio_element = self._driver.find_element(By.XPATH, selector)
                    bio_text = bio_element.text.strip()
                    if bio_text:
                        rprint(f"[green]  ‚úì ƒê√£ l·∫•y ƒë∆∞·ª£c ti·ªÉu s·ª≠ t·ª´ trang about: {bio_text}[/green]")
                        return bio_text
                except:
                    continue
                    
            rprint("[yellow]  ! Kh√¥ng t√¨m th·∫•y ti·ªÉu s·ª≠[/yellow]")
            return ""
        except Exception as e:
            rprint(f"[yellow]  ! L·ªói khi l·∫•y ti·ªÉu s·ª≠: {str(e)}[/yellow]")
            return ""

    def get_about_info(self, about_sections):
        about_data = {}
        
        for section in about_sections:
            try:
                rprint(f"\n[bold cyan]‚Üí ƒêang crawl section: {section}[/bold cyan]")
                
                if self.user_id.isdigit():
                    url = f"{self.profile_url}&sk={section}"
                else:
                    url = f"{self.profile_url}/{section}"
                    
                rprint(f"[blue]  - Truy c·∫≠p: {url}[/blue]")
                self._driver.get(url)
                sleep(1)
                
                section_data = {
                    "title": "",
                    "url": url,
                    "content": {}
                }
                
                # L·∫•y t√™n Facebook t·ª´ h√†m get_name()
                try:
                    name = self.get_name()
                    if name:
                        section_data["title"] = name
                        rprint(f"[green]  ‚úì Ti√™u ƒë·ªÅ section: {section_data['title']}[/green]")
                    else:
                        section_data["title"] = section.replace("about_", "").replace("_", " ").title()
                        rprint(f"[yellow]  ! Kh√¥ng l·∫•y ƒë∆∞·ª£c t√™n Facebook, s·ª≠ d·ª•ng ti√™u ƒë·ªÅ m·∫∑c ƒë·ªãnh: {section_data['title']}[/yellow]")
                except Exception as e:
                    section_data["title"] = section.replace("about_", "").replace("_", " ").title()
                    rprint(f"[yellow]  ! L·ªói khi l·∫•y t√™n Facebook: {str(e)}, s·ª≠ d·ª•ng ti√™u ƒë·ªÅ m·∫∑c ƒë·ªãnh: {section_data['title']}[/yellow]")

                content_elements = self._driver.find_elements(
                    By.XPATH,
                    "//div[contains(@class, 'x1hq5gj4') or contains(@class, 'x1gan7if')]//span[contains(@class, 'x193iq5w')]"
                )
                
                rprint(f"[blue]  - T√¨m th·∫•y {len(content_elements)} ph·∫ßn t·ª≠ ch·ª©a th√¥ng tin[/blue]")
                
                current_category = None
                for element in content_elements:
                    text = element.text.strip()
                    if not text:
                        continue
                        
                    if element.get_attribute("role") == "heading" or text.endswith(":"):
                        current_category = text.rstrip(":")
                        if current_category not in section_data["content"]:
                            section_data["content"][current_category] = []
                            rprint(f"[blue]    + Category: {current_category}[/blue]")
                    elif current_category:
                        if text not in ["Xem th√™m", "·∫®n b·ªõt"]:
                            section_data["content"][current_category].append(text)
                            rprint(f"[cyan]      ¬∑ {text}[/cyan]")
                    else:
                        if "general" not in section_data["content"]:
                            section_data["content"]["general"] = []
                        if text not in ["Xem th√™m", "·∫®n b·ªõt"]:
                            section_data["content"]["general"].append(text)
                            rprint(f"[cyan]      ¬∑ {text}[/cyan]")
                
                about_data[section] = section_data
                rprint(f"[green]  ‚úì Ho√†n th√†nh crawl section: {section}[/green]")
                
            except Exception as e:
                rprint(f"[red]  ! L·ªói khi crawl section {section}: {str(e)}[/red]")
                about_data[section] = {
                    "title": section.replace("about_", "").replace("_", " ").title(),
                    "url": url,
                    "content": {"error": str(e)}
                }
            
            sleep(1)
        
        return about_data   

    def crawl_profile(self):
        data = {}
        
        try:
            rprint("[bold]Step 1 of 3 - Load cookies[/bold]")
            self._load_cookies_and_refresh_driver()
            
            rprint("\n[bold]Step 2 of 3 - L·∫•y th√¥ng tin c∆° b·∫£n[/bold]")
            rprint(f"[blue]‚Üí Truy c·∫≠p profile: {self.profile_url}[/blue]")
            self._driver.get(self.profile_url)
            sleep(2)
            
            rprint("\n[cyan]‚Üí Thu th·∫≠p th√¥ng tin c∆° b·∫£n...[/cyan]")
            data['title'] = self.get_name()
            data['profile_picture'] = self.get_profile_picture()
            data['cover_photo'] = self.get_cover_photo()
            data['followers_count'] = self.get_followers_count()
            data['friends_count'] = self.get_friends_count()
    
            
            data['bio'] = self.get_bio()
            
            rprint("\n[bold]Step 3 of 3 - Crawl th√¥ng tin about[/bold]")
            about_sections = [
                'about',
                'about_work_and_education',
                'about_places',
                'about_contact_and_basic_info',
                'about_privacy_and_legal_info',
                'about_profile_transparency',
                'about_family_and_relationships',
                'about_details',
                'about_life_events'
            ]
            
            data['about_info'] = self.get_about_info(about_sections)
            
            rprint("\n[bold green]‚úì ƒê√£ ho√†n th√†nh crawl profile![/bold green]")
            return data
            
        except Exception as e:
            rprint(f"[red]! L·ªói khi crawl profile: {e}[/red]")
            raise
        finally:
            self._driver.quit()

def get_friends_data_folders():
    friends_data_folders = []
    
    project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))
    
    for root, dirs, _ in os.walk(project_root):
        matching_dirs = [d for d in dirs if d.startswith('friends_data_')]
        for dir_name in matching_dirs:
            full_path = os.path.join(root, dir_name)
            relative_path = os.path.relpath(full_path, os.getcwd())
            friends_data_folders.append(relative_path)
    
    return friends_data_folders

def select_folder(folders):
    if not folders:
        rprint("[red]Kh√¥ng t√¨m th·∫•y th∆∞ m·ª•c friends_data_* n√†o trong d·ª± √°n[/red]")
        return None
        
    rprint("\n[bold]Danh s√°ch th∆∞ m·ª•c friends_data_* c√≥ s·∫µn:[/bold]")
    for idx, folder in enumerate(folders, 1):
        rprint(f"{idx}. {folder}")
    
    while True:
        try:
            choice = int(input("\nCh·ªçn s·ªë th·ª© t·ª± th∆∞ m·ª•c (1-{}): ".format(len(folders))))
            if 1 <= choice <= len(folders):
                selected = folders[choice - 1]
                rprint(f"[green]ƒê√£ ch·ªçn: {selected}[/green]")
                return selected
            rprint("[yellow]Vui l√≤ng ch·ªçn s·ªë t·ª´ 1 ƒë·∫øn[/yellow]", len(folders))
        except ValueError:
            rprint("[yellow]Vui l√≤ng nh·∫≠p m·ªôt s·ªë h·ª£p l·ªá[/yellow]")

def get_json_files(folder):
    return glob.glob(os.path.join(folder, "*.json"))

def collect_profile_ids(tree_data: Dict[str, Any]) -> List[Dict[str, Any]]:
    profiles = []
    
    def traverse_tree(node: Dict[str, Any]):
        if not isinstance(node, dict) or not node:
            rprint("[yellow]B·ªè qua node kh√¥ng h·ª£p l·ªá[/yellow]")
            return
            
        if "id" in node:
            profile_info = {
                "id": node["id"],
                "profile_url": node.get("profile_url", ""),
                "layer": node.get("layer", 0),
                "relationship_mark": node.get("relationship_mark", "root")
            }
            profiles.append(profile_info)
            rprint(f"[green]ƒê√£ thu th·∫≠p th√¥ng tin profile: {profile_info['id']} (Layer: {profile_info['layer']})[/green]")
        
        children = node.get("children", [])
        if isinstance(children, list):
            rprint(f"[cyan]Duy·ªát qua {len(children)} node con[/cyan]")
            for child in children:
                traverse_tree(child)
    
    if not isinstance(tree_data, dict) or "tree_data" not in tree_data:
        rprint("[red]C·∫•u tr√∫c d·ªØ li·ªáu kh√¥ng h·ª£p l·ªá: Thi·∫øu key 'tree_data'[/red]")
        return []
        
    rprint("\n[bold]B·∫Øt ƒë·∫ßu duy·ªát c√¢y d·ªØ li·ªáu[/bold]")
    traverse_tree(tree_data["tree_data"])
    rprint(f"[bold green]ƒê√£ thu th·∫≠p xong {len(profiles)} profiles[/bold green]")
    return profiles

def read_json_file(json_file):
    with open(json_file, 'r', encoding='utf-8') as f:
        return json.load(f)

def save_data(crawled_data: Dict[str, Any], original_data: Dict[str, Any], output_folder: str, base_filename: str):
    rprint("\n[bold]üì¶ B·∫Øt ƒë·∫ßu qu√° tr√¨nh l∆∞u d·ªØ li·ªáu[/bold]")

    def update_tree_data(node: Dict[str, Any]):
        if not isinstance(node, dict) or not node:
            rprint("‚ö†Ô∏è [yellow]B·ªè qua node kh√¥ng h·ª£p l·ªá[/yellow]")
            return

        if "id" in node:
            node_id = node["id"]
            if node_id in crawled_data:
                node.update(crawled_data[node_id])
                rprint(f"‚úÖ [green]ƒê√£ c·∫≠p nh·∫≠t d·ªØ li·ªáu cho node: {node_id}[/green]")

        children = node.get("children", [])
        if isinstance(children, list):
            rprint(f"üîÅ [cyan]C·∫≠p nh·∫≠t {len(children)} node con[/cyan]")
            for child in children:
                update_tree_data(child)

    if not isinstance(original_data, dict) or "tree_data" not in original_data:
        rprint("‚ùå [red]C·∫•u tr√∫c d·ªØ li·ªáu g·ªëc kh√¥ng h·ª£p l·ªá: Thi·∫øu key 'tree_data'[/red]")
        return

    rprint("\nüå≥ [bold]B·∫Øt ƒë·∫ßu c·∫≠p nh·∫≠t d·ªØ li·ªáu v√†o c√¢y[/bold]")
    update_tree_data(original_data["tree_data"])

    try:
        max_layers = original_data['max_layers']
    except KeyError as e:
        raise ValueError(f"‚ùå Thi·∫øu tr∆∞·ªùng {e} trong d·ªØ li·ªáu g·ªëc, kh√¥ng th·ªÉ l∆∞u file chu·∫©n h√≥a!")

    tree_data = original_data["tree_data"]
    normalized_data = {
        'root_user': original_data.get('root_user', tree_data.get('id', '')),
        'max_layers': max_layers,
        'crawled_at': original_data.get('crawled_at', ''),
        'total_accounts': original_data.get('total_accounts', 0),
        'tree_data': tree_data
    }

    os.makedirs(output_folder, exist_ok=True)

    existing_files = os.listdir(output_folder)
    pattern = re.compile(r'data_(\d+)_' + re.escape(base_filename) + r'\.json$')
    max_idx = 0
    for fname in existing_files:
        m = pattern.match(fname)
        if m:
            idx = int(m.group(1))
            max_idx = max(max_idx, idx)
    new_idx = max_idx
    new_filename = f"data_{new_idx}_{base_filename}"
    output_path = os.path.join(output_folder, new_filename)

    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(normalized_data, f, ensure_ascii=False, indent=2)

    rprint(f"\nüíæ [bold green]ƒê√£ l∆∞u d·ªØ li·ªáu th√†nh c√¥ng v√†o {output_path}[/bold green]")

def main():
    rprint("[bold cyan]B·∫Øt ƒë·∫ßu ch∆∞∆°ng tr√¨nh ch√≠nh...[/bold cyan]")
    
    rprint("[bold]B∆∞·ªõc 1: T√¨m ki·∫øm th∆∞ m·ª•c friends_data_*[/bold]")
    folders = get_friends_data_folders()
    if not folders:
        print("Kh√¥ng t√¨m th·∫•y th∆∞ m·ª•c friends_data_*")
        return
    
    rprint("[bold]B∆∞·ªõc 2: Ch·ªçn th∆∞ m·ª•c ƒë·ªÉ x·ª≠ l√Ω[/bold]")
    selected_folder = select_folder(folders)
    if not selected_folder:
        return
    
    rprint("[bold]B∆∞·ªõc 3: T√¨m ki·∫øm file JSON trong th∆∞ m·ª•c ƒë√£ ch·ªçn[/bold]")
    json_files = get_json_files(selected_folder)
    if not json_files:
        print(f"Kh√¥ng t√¨m th·∫•y file JSON trong th∆∞ m·ª•c {selected_folder}")
        return
    
    rprint("[bold]B∆∞·ªõc 4: ƒê·ªçc d·ªØ li·ªáu t·ª´ file JSON[/bold]")
    json_data = read_json_file(json_files[0])
    
    rprint("[bold]B∆∞·ªõc 5: Thu th·∫≠p th√¥ng tin profile t·ª´ d·ªØ li·ªáu JSON[/bold]")
    profiles = collect_profile_ids(json_data)
    
    profiles.sort(key=lambda x: x["layer"])
    
    rprint("\n[bold]Th√¥ng tin crawl:[/bold]")
    rprint(f"T·ªïng s·ªë profile c·∫ßn crawl: {len(profiles)}")
    for layer in range(max(p["layer"] for p in profiles) + 1):
        layer_profiles = [p for p in profiles if p["layer"] == layer]
        rprint(f"T·∫ßng {layer}: {len(layer_profiles)} profile")
    
    rprint("[bold]B∆∞·ªõc 6: B·∫Øt ƒë·∫ßu qu√° tr√¨nh crawl d·ªØ li·ªáu[/bold]")
    crawled_data = {}
    for profile in profiles:
        profile_id = profile["id"]
        rprint(f"\n[bold]ƒêang crawl profile {profile_id}[/bold]")
        rprint(f"Layer: {profile['layer']}")
        rprint(f"Relationship mark: {profile['relationship_mark']}")
        
        try:
            crawler = FacebookProfileCrawler(profile_id)
            profile_data = crawler.crawl_profile()
            profile_data["layer"] = profile["layer"]
            profile_data["relationship_mark"] = profile["relationship_mark"]
            crawled_data[profile_id] = profile_data
        except Exception as e:
            rprint(f"[red]L·ªói khi crawl profile {profile_id}: {e}[/red]")
            crawled_data[profile_id] = {
                "error": str(e),
                "layer": profile["layer"],
                "relationship_mark": profile["relationship_mark"]
            }
        
        sleep(1)
    
    rprint("[bold]B∆∞·ªõc 7: L∆∞u d·ªØ li·ªáu ƒë√£ crawl[/bold]")
    output_folder = selected_folder
    filename = os.path.basename(json_files[0])
    save_data(crawled_data, json_data, output_folder, filename)

    rprint("[bold green]Ch∆∞∆°ng tr√¨nh ƒë√£ ho√†n th√†nh![bold green]")

if __name__ == "__main__":
    main()